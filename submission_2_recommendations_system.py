# -*- coding: utf-8 -*-
"""Submission 2 - Recommendations System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UpW-594QTGqrw5_Gin0go57RLfU4T1zd

# Data Loading

### Download dataset dari kaggle

> Pengambilan dataset dari kaggle dilakukan menggunakan API kaggle. Dataset diunduh dengan menuliskan kode dengan format nama_pembuat/nama_dataset. Setelah pengunduhan dataset berhasil, dataset kemudian di unzip dan siap digunakan.

> Pertama-tama, mari install package kaggle terlebih dahulu
"""

!pip install kaggle

"""> Kemudian mari atur API key kaggle.json"""

!mkdir ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

"""> Setelah itu, mari cari dataset yang diinginkan. Saya memilih dataset dengan keyword pencarian 'recommend' untuk proyek sistem rekomendasi saya"""

!kaggle datasets list -s recommend

"""> Pilih salah satu dataset yang muncul dan download dengan format nama_pembuat/nama_dataset. Dataset yang saya pilih adalah dataset yang berjudul *Movie Recommender System Dataset*."""

!kaggle datasets download gargmanas/movierecommenderdataset  --unzip

"""### Import Library & Data Overview"""

# library untuk data
import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
import re
import requests
from PIL import Image

# library untuk content based filtering
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import ndcg_score

# library untuk collaborative filtering
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path

movies = pd.read_csv('/content/movies.csv')
ratings = pd.read_csv('/content/ratings.csv')

print(f'Jumlah data movie : {len(movies.movieId.unique())}')
print(f'Jumlah data penilaian movie : {len(ratings.movieId.unique())}')
print(f'Jumlah data pengguna yang memberikan penilaian movie : {len(ratings.userId.unique())}')

"""> Berdasarkan output di atas, maka dapat disimpulkan bahwa data telah berhasil di *import* ke kernel. Setelah ini adalah tahap *univariate exploratory data analysis*

# Exploratory Data Analysis

> Variabel-variabel pada *Movie Recommender System Dataset* adalah sebagai berikut:
- movies: merupakan data yang berisi mengenai informasi movie
- ratings: merupakan data yang berisi mengenai informasi penilaian movie

> Pada tahap ini, akan dilakukan analisis pada masing-masing variabel *Movie Recommender System Dataset*. Tahap pertama adalah melakukan deskripsi masing-masing variabel.

## Deskripsi Variabel

### Variabel movies
"""

movies.info()

"""> Berdasarkan output di atas, dapat diketahui bahwa file movies.csv memiliki 9742 entri. Variabel-variabel dalam movies.csv adalah sebagai berikut:
- movieId: id movie
- title: judul movie
- genres: daftar genre movie

### Variabel ratings
"""

ratings.info()

"""> Berdasarkan output di atas, dapat diketahui bahwa file ratings.csv memiliki 100836 entri. Variabel-variabel dalam ratings.csv adalah sebagai berikut:
- userId: id pengguna
- movieId: id movie
- rating: jumlah penilain movie terhadap pengguna
- timestamp: waktu ketika entri diciptakan yang berbentuk detik sejak 1 Januari 1970

> Tahap selanjutnya setelah deskripsi variabel adalah *data cleaning*

## Data Cleaning

> Tahap pertama adalah melakukan penggabungan antara variabel movies dan ratings.
"""

df = ratings.merge(movies, on="movieId")
df.head()

"""> Selanjutnya mari kita cek terlebih dahulu deskripsi dari df."""

df.info()

"""> Berdasarkan info diatas, maka dapat disimpulkan dataframe df, hasil penggabungan antara movies dan ratings sekarang memiliki 6 kolom yang semuanya berjumlah 100836 entri. Selanjutnya mari kita cek deskripsi kolom numerik dari dataframe df."""

df.describe()

"""> Berdasarkan deskripsi kolom numerik, sepertinya tidak ada yang bermasalah. Selanjutnya kita cek apakah terdapat *missing value*"""

df.isnull().sum()

"""> Sepertinya juga tidak ada *missing value*. Selanjutnya kita periksa kolom title"""

df.title

"""> Berdasarkan output di atas, judul movie masih digabung dengan tahun movie. Mari pisahkan judul dan tahun ini."""

df['year'] = df.title.str.extract('.*\((.*)\).*')
df['title'] = df.title.str.split('(').str[0].str[:-1]
df.head()

"""> Sekarang coba cek kembali informasi mengenai dataset"""

df.info()

"""> Berdasarkan output diatas, ternyata masih ada keanehan, yaitu pada fitur year. Seharusnya fitur tersebut bertipe int64 karena merupakan kolom numerik, tetapi bertipe object, mari periksa fitur tersebut."""

df.year.unique()

"""> Ternyata ada kolom yang terisi dengan nan dan 2006-2007. mari atasi nilai nan terlebih dahulu"""

df.dropna(inplace=True)

"""> Setelah itu mari atasi tahun yang bernilai 2006-2007"""

df.drop(index=df[df.year.str.len() > 5].index, inplace=True)

"""> Kemudian kita ubah tipe data kolom year menjadi int64"""

df.year = df.year.astype('int64')

"""> Mari cek kembali deskripsi df"""

df.info()

"""Sepertinya dataset sekarang sudah rapi dan bersih. Tahap selanjutnya adalah Analisis fitur df.

## Analisis

### Jumlah frekuensi ratings
"""

rating_val_count = df.rating.value_counts()
fig = px.bar(rating_val_count, x=rating_val_count.index, y=rating_val_count, text=rating_val_count,
             labels={
                 "index": "Ratings",
                 'y': 'Number of Ratings'},
             color=rating_val_count,
             width=800, height=400
             )
fig.update_traces(textposition='outside')
fig.update_layout(title_text='Frequency of the Ratings',
                  title_x=0.5, title_font=dict(size=24))
fig.update_traces(marker=dict(line=dict(color='#000000', width=2)))
fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'})
fig.show()

"""> Berdasarkan visualisasi diatas, kebanyakan pengguna melakukan penilaian movie sebanyak 4. Dapat disimpulkan juga, fitur rating berkisar antara 1 hingga 5

### Jumlah frekuensi ratings per movie
"""

_ = df.title.value_counts()
fig = px.histogram(_, x=_, opacity=0.85, marginal='box',
                   labels={
                       'x': 'Number of Ratings taken'},
                   width=800, height=400)
fig.update_traces(marker=dict(line=dict(color='#000000', width=1)))
fig.update_layout(title_text='Distribution of the Number of Ratings taken by the Movies',
                  title_x=0.5, title_font=dict(size=20))
fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'})
fig.show()

"""> Berdasarkan visualisasi diatas, dapat disimpulkan bahwa rata-rata jumlah movie yang diberikan rating oleh pengguna adalah sekitar 3000. Ada juga jumlah movie yang diberikan rating dibawah 10. Movie tersebut akan saya drop karena sepertinya tidak berpengaruh terhadap sistem rekomendasi."""

movieFrequency_greater_10 = df['movieId'].value_counts()[df['movieId'].value_counts() >= 10].index
df = df[df.movieId.isin(movieFrequency_greater_10)]

print("Jumlah minimal rated movies setelah proses drop:", df.title.value_counts().nsmallest(5))

"""> Dapat terlihat bahwa sekarang rated movie terkecil adalah Fast Five, yaitu berjumlah 10

### Jumlah frekuensi movie berdasarkan genre
"""

genres_value_counts = df['genres'].str.split('|', expand=True).stack().value_counts()
fig = px.bar(genres_value_counts, x=genres_value_counts.index, y=genres_value_counts, text=genres_value_counts,
             labels={
                 "index": "Genres",
                 'y': 'Frequency'},
             color=genres_value_counts,
             width=800, height=400
             )
fig.update_traces(textposition='outside')
fig.update_layout(title_text='Top Frequent the Movie Genres',
                  title_x=0.5, title_font=dict(size=24))
fig.update_traces(marker=dict(line=dict(color='#000000', width=2)))
fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'})
fig.show()

"""> Berdasarkan visualisasi diatas, maka dapat terlihat genre drama adalah genre dengan jumlah terbanyak dan paling sedikit adalah documentary.

> Berdasarkan hasil analisis, maka dapat disimpulkan beberapa hal berikut
- Jarak rating berkisar antara 0.5 hingga 5
- Rating terbanyak berada pada jumlah rating 4
- Rata-rata user melakukan rating terhadap movie adalah sekitar 3000 kali
- Movie yang diberi rate kurang dari 10 kali dihapus karena tidak akan berpengaruh terhadap sistem rekomendasi
- Movie bergenre drama adalah genre movie yang paling banyak, sedangkan movie bergenre documentary adalah genre movie yang paling sedikit.

> Sebelum melangkah ke tahap selanjutnya, mari cek terlebih dahulu kondisi dataset sekarang.
"""

df.info()

df.describe()

"""> Fyuhhh... sepertinya sekarang dataset sudah aman, rapi, dan bersih sehingga dapat disimpulkan bahwa tahap EDA kini telah selesai. Selanjutnya mari ke tahap selanjutnya, yaitu *data preparation*

# Data Preparation

> Pada proyek ini, terdapat tiga jenis sistem rekomendasi yang ingin saya bangun. Sistem rekomendasi tersebut adalah sebagai berikut
- Non-personalized recommendation
- Content-based recommendation
- Collaborative recommendation

> Masing-masing dari ketiga sistem rekomendasi di atas membutuhkan kombinasi dataset dengan formasi yang berbeda. Fokus dari tahap ini adalah membangun dataset untuk ketiga sistem rekomendasi di atas.

## Preprocessing

### Non-personalized recommendation

> Non-personalized recommendation merupakan sistem rekomendasi yang tidak membutuhkan fitur spesifik. Sistem rekomendasi ini biasanya digunakan pada *homepage* aplikasi. Sebagai contoh adalah produk terpopuler pada *homepage* aplikasi *e-commerce*. Dataset yang dibutuhkan pada sistem rekomendasi ini adalah dataset yang memiliki sorting descending dalam fitur populer. Untuk membuat dataset sesuai dengan kondisi tersebut, maka fitur yang dibutuhkan hanyalah fitur title dan rating saja. Langsung saja kita buat dataframe baru
"""

non_personalized_df = df[['title', 'rating']].copy()
non_personalized_df.head()

"""> Berdasarkan output diatas, kita sudah berhasil mengekstrak fitur title dan rating pada dataset non_personalized_df untuk sistem rekomendasi pertama kita. Namun, perlu dilihat bahwa sepertinya terdapat duplikat entri, oleh karena itu, mari bersihkan terlebih dahulu hal tersebut."""

non_personalized_df.drop_duplicates(inplace=True)
non_personalized_df.head()

"""> Okeee...sudah beres. Sekarang mari lanjut ke tahap selanjutnya, yaitu membuat dataset untuk sistem rekomendasi kedua, content based recommendation

### Content-based recommendation

> Content-based recommendation merupakan sebuah sistem rekomendasi yang menggunakan kesamaan fitur sebagai dasar rekomendasi. Sistem rekomendasi ini merekomendasikan item yang mirip dengan yang direferensikan pengguna di masa lalu. Kategori item yang mirip dalam kasus proyek ini dapat direferensikan dari genre movie. Untuk memenuhi kondisi tersebut, dataset yang dibutuhkan pada sistem rekomendasi ini adalah dataset yang berisi mengenai informasi genre movie. Untuk memenuhi kondisi tersebut, maka fitur yang dibutuhkan adalah fitur genre dari dataframe df. Langsung gasken buat dataset genre :).
"""

title_genres = df[['movieId' ,'title', 'genres', 'year']].drop_duplicates(subset='title').reset_index().drop(columns='index')
genres = []
for genre in title_genres.genres:
    genres = genres + [(genre.replace(' ', '_').replace('|', ' '))]
pd.Series(genres).head(5)

"""> Dataset untuk content-based recommendation telah selesai dibuat, selanjutnya mari buat dataset untuk collaborative recommendation

### Collaborative Recommendation

> Collaborative recommendation merupakan sistem rekomendasi yang menggunakan pendapat komunitas atau pengguna terhadap suatu produk sebagai dasar rekomendasi. Sistem rekomendasi ini akan merekomendasikan produk yang kira-kira disukai pengguna lain berdasarkan pendapat pengguna lain dengan kesukaan yang sama. Dataset yang dibutuhkan dalam sistem rekomendasi ini adalah dataset yang memuat informasi mengenai rating movie serta beberapa fitur seperti id user dan id movie. Tahap pertama yang dilakukan untuk memenuhi kondisi tersebut adalah membuat dataset collaborative yang berisi id user, id movie, dan rating.
"""

collaborative = df[['userId', 'movieId', 'rating']].copy()

"""> Dataframe collaborative berhasil dibuat, lanjut ke tahap selanjutnya, yaitu melakukan encode id user"""

user_ids = collaborative['userId'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

"""> Selanjutnya, lakukan hal yang sama pada id movie"""

movie_ids = collaborative['movieId'].unique().tolist()
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

"""> Selanjutnya, mari lakukan mapping encode diatas pada dataframe collaborative"""

collaborative['user'] = collaborative['userId'].map(user_to_user_encoded)
collaborative['movie'] = collaborative['movieId'].map(movie_to_movie_encoded)

"""> Sebelum lanjut ke tahap selanjutnya, mari coba cek hasil dataframe collaborative sekarang"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)
 
# Mendapatkan jumlah movie
num_movie = len(movie_encoded_to_movie)
print(num_movie)
 
# Nilai minimum rating
min_rating = min(collaborative['rating'])
 
# Nilai maksimal rating
max_rating = max(collaborative['rating'])
 
print('Number of User: {}, Number of movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movie, min_rating, max_rating
))

"""> Okee... selanjutnya mari kita acak sampel dataframe collaborative sebelum melakukan pembagian data"""

collaborative = collaborative.sample(frac=1, random_state=42)
collaborative

"""> Selanjutnya, mari kita lakukan pembagian sampel menjadi data latih dan data validasi dengan rasio 90:10. Perbandingan tersebut dilakukan berdasarkan pertimbangan mengenai jumlah entri dalam dataset yang sangat banyak."""

x = collaborative[['user', 'movie']].values
y = collaborative['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.9 * collaborative.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""> Sekarang data untuk collaborative recommendations telah siap. Tahap selanjutnya adalah modelling

# Modelling

> Pada tahap ini akan dibuat fungsi dan class yang berperan sebagai model untuk sistem rekomendasi yang akan diterapkan dalam proyek ini. Masing-masing fungsi dan class tersebut akan menggunakan dataset khusus yang telah dibuat dan disiapkan pada tahap *data preparation*.

## Non personalized recommendation

> Pada sistem rekomendasi ini, akan dibuat sebuah fungsi yang bertugas untuk memilih buku terpopuler. Buku terpopuler ditentukan berdasarkan jumlah rating pengguna terhadap suatu buku dan rata-rata ratingnya. Penjelasan detail mengenai algoritma fungsi akan dibahas di laporan proyek.
"""

def popular_movies(data,n=100):
    rating_count=data.groupby("title").count()["rating"].reset_index()
    rating_count.rename(columns={"rating":"numberOfVotes"},inplace=True)
    
    rating_average=data.groupby("title")["rating"].mean().reset_index()
    rating_average.rename(columns={"rating":"averageRatings"},inplace=True)
    
    popularMovies=rating_count.merge(rating_average,on="title")
    
    def weighted_rate(x):
        v=x["numberOfVotes"]
        R=x["averageRatings"]
        
        return ((v*R) + (m*C)) / (v+m)
    
    C=popularMovies["averageRatings"].mean()
    m=popularMovies["numberOfVotes"].quantile(0.90)
    
    popularMovies["popularity"]=popularMovies.apply(weighted_rate,axis=1)
    popularMovies=popularMovies.sort_values(by="popularity",ascending=False)
    return popularMovies[["title","numberOfVotes","averageRatings","popularity"]].reset_index(drop=True).head(n)

"""> Model untuk non personalized recommendation telah dibuat, selanjutnya adalah pembuatan model untuk content-based recommendation

## Content-based recommendation

> Sistem rekomendasi ini dibangun dengan menggunakan teknik count vectorizer. Teknik tersebut digunakan untuk menemukan representasi fitur penting berdasarkan frekuensi kemunculan nilai terbanyak. Setelah representasi fitur dan korelasi antar fitur sudah diketahui, akan dicari derajat kesamaan antar fitur menggunakan teknik cosine similarity. Penjelasan detail mengenai algoritma fungsi akan dijelaskan pada laporan proyek.

> Langkah pertama dalam pembuatan model untuk content based adalah membuat count vectorizer movie berdasarkan genres.

### Count vectorizer
"""

cv = CountVectorizer()
sparse_m = cv.fit_transform(genres)
features = cv.get_feature_names_out()
count_table = pd.DataFrame(sparse_m.toarray(), columns = features, index = title_genres.title)
print(features)
count_table

"""> Berdasarkan output diatas, representasi fitur movie berdasarkan genres telah berhasil dibuat. Langkah selanjutnya adalah membuat fungsi yang berperan sebagai model untuk menentukan derajat kesamaan antar fitur.

### Cosine similarity
"""

def content_based_genre_by_user(userId):
    by_time = df[df.userId == userId].sort_values(by='timestamp', ascending=False).iloc[:3]
    user_pref = pd.concat([df, by_time.title.apply(lambda title: count_table.loc[title, :])], axis = 1)
    total_user_pref = user_pref.groupby('userId').sum().iloc[:,4:]
    similarity = {}
    for row in count_table.index:
        similarity[row] = float( cosine_similarity( pd.DataFrame(count_table.loc[row]).T, pd.DataFrame(total_user_pref.loc[userId]).T ) )
    similarity = pd.Series(similarity)
    return title_genres[title_genres.title.isin(similarity.sort_values(ascending=False)[:10].index)]

"""> Model untuk content-based recommendation telah dibuat, selanjutnya adalah pembuatan model untuk collaborative recommendation

## Collaborative recommendation

> Sistem rekomendasi ini dibangun dengan membuat class baru dengan Keras model. Class baru tersebut bernama CollRecNet. Penjelasan lebih detail mengenai CollRecNet akan dijelaskan pada laporan proyek. Langkah pertama, mari buat terlebih dahulu class CollRecNet.
"""

class CollRecNet(tf.keras.Model):

  def __init__(self, num_users, num_movies, embedding_size, **kwargs):
    super(CollRecNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movies = num_movies
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.movies_embedding = layers.Embedding(
        num_movies,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movies_bias = layers.Embedding(num_movies, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    movies_vector = self.movies_embedding(inputs[:, 1])
    movies_bias = self.movies_bias(inputs[:, 1])

    dot_user_movies = tf.tensordot(user_vector, movies_vector, 2)

    x = dot_user_movies + user_bias + movies_bias
    
    return tf.nn.sigmoid(x)

"""> Selanjutnya mari kita compile model CollRecNet"""

model = CollRecNet(num_users, num_movie, 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""> Langkah berikutnya adalah memulai proses training"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 20,
    validation_data = (x_val, y_val)
)

"""> Fyuuhhhh... akhirnya modelling untuk seluruh sistem remondasi berhasil dilakukan. Langkah selanjutnya adalah evaluasi

# Evaluasi

## Non personalized recommendation

> Pada tahap ini, akan didapatkan hasil rekomendasi dari fungsi popular_books yang telah dibuat
"""

popular_movies(non_personalized_df, n=10)

"""> Berdasarkan output diatas, dapat terlihat daftar movie terpopuler. Movie terpopuler adalah Elite Squad dengan nilai popularity 3.48. Hasil dari sistem rekomendasi ini biasanya akan digunakan untuk *homepage* aplikasi sesuai dengan yang telah dijelaskan pada tahap *data preparation*. Sistem rekomendasi ini merekomendasikan item tidak berdasarkan suatu fitur, sehingga pada sistem rekomendasi ini tidak dilakukan pengukuran performa evaluasi.

> Langkah selanjutnya adalah melakukan evaluasi terhadap content based recommendation

## Content based recommendation

> Pertama, mari cek dulu, hasil rekomendasi dari fungsi content\_based\_genres\_by\_user
"""

content_based_genre_by_user(125)

"""> Output diatas merupakan daftar movie yang direkomendasikan kepada pengguna yang memiliki id 125. Movie diatas direkomendasikan berdasarkan genre movie yang disukai oleh pengguna. Setelah ini akan dilakukan evaluasi menggunakan matriks *normalized discount cumulative gain* atau biasa disebut NDCG.

### Evaluasi content based menggunakan NDCG

> Pertama, mari kita buat fungsi untuk mendapatkan daftar movie yang pernah diberikan rating oleh pengguna berdasarkan preferensi masa lalu.
"""

def get_movie_preference_by_user(userId):
  by_time = df[df.userId == userId].sort_values(by='timestamp', ascending=False).iloc[3:6]
  user_pref = pd.concat([df, by_time.title.apply(lambda title: count_table.loc[title, :])], axis = 1)
  total_user_pref = user_pref.groupby('userId').sum().iloc[:,4:]
  similarity = {}
  for row in count_table.index:
      similarity[row] = float( cosine_similarity( pd.DataFrame(count_table.loc[row]).T, pd.DataFrame(total_user_pref.loc[userId]).T ) )
  similarity = pd.Series(similarity)
  return title_genres[title_genres.title.isin(similarity.sort_values(ascending=False)[:10].index)]

"""> Setelah membuat fungsi diatas, mari kita coba hasil preferensi movie pengguna."""

get_movie_preference_by_user(125)

"""> Berdasarkan hasil output diatas, maka terlihat bahwa terdapat beberapa movie yang memiliki genre mirip seperti hasil rekomendasi diatas. Baik, langkah selanjutnya mari kita hitung NDCG skor dengan melakukan perbandingan hasil rekomendasi dan preferensi pengguna. Untuk memudahkan, mari kita buat fungsi ndcg_result."""

def ndcg_result(userId):
  rec_2 = np.asarray([content_based_genre_by_user(userId).movieId.values])
  rec_1 = np.asarray([get_movie_preference_by_user(userId).movieId.values])
  print(f'NDCG score : {ndcg_score(rec_1, rec_2)}')

"""> Mari kita tes hasil evaluasi content based terhadap beberapa pengguna"""

ndcg_result(125)

ndcg_result(45)

ndcg_result(5)

"""> Berdasarkan hasil diatas, dapat terlihat bahwa NDCG skor memiliki rate yang tinggi (paling kecil 0.82) sehingga dapat disimpulakn bahwa content based recommendation yang dibuat cukup mengesankan.

> Evaluasi pada content based telah dilakukan, selanjutnya adalah evaluasi pada collaborative recommendation

## Collaborative recommendation

> Pada sistem rekomendasi ini, matriks evaluasi yang digunakan adalah *root square mean error*. Hasil visualisasi matriks pada model dapat dilihat pada grafik di bawah

### Visualisasi metriks
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""> Berdasarkan hasil grafik visualisasi di atas, maka dapat disimpulkan bahwa proses training model cukup smooth dan model konvergen pada epochs sekitar 10. Dari proses ini, diperoleh nilai error akhir sebesar sekitar 0.183 dan error pada data validasi sebesar 0.194. Nilai tersebut cukup bagus untuk collaborative sistem yang dibuat. Kemudian mari kita cek hasil rekomendasi model.

### Hasil collaborative recommendation

> Pertama, mari buat daftar movie yang belum pernah ditonton oleh pengguna dengan sampel pengguna akan diambil secara acak.
"""

user_id = df.userId.sample(1).iloc[0]
movie_watched_by_user = df[df.userId == user_id]

movie_not_watched = collaborative[~collaborative['movieId'].isin(movie_watched_by_user.movieId.values)]['movieId']
movie_not_watched = list(
    set(movie_not_watched)
    .intersection(set(movie_to_movie_encoded.keys()))
)

movie_not_watched = [[movie_to_movie_encoded.get(x)] for x in movie_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_watched), movie_not_watched)
)

"""> Kemudian mari dapatkan hasil rekomendasi dengan menggunakan fungsi predict"""

recommendation_results = model.predict(user_movie_array).flatten()

top_recommendation_results_indices = recommendation_results.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_watched[x][0]) for x in top_recommendation_results_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Movie with high ratings from user')
print('----' * 8)

top_movie_user = (
    movie_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)

movie_df_rows = movies[movies['movieId'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(f'Title : {row.title}, Genres : {row.genres}')

print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)

recommended_movies = movies[movies['movieId'].isin(recommended_movie_ids)]
for row in recommended_movies.itertuples():
    print(f'Title : {row.title}, Genres : {row.genres}')

"""> Jika dilihat dari genre movie, genre movie yang direkomendasikan lumayan mirip dengan movie yang diberi rating tinggi oleh pengguna sehingga dapat disimpulkan bahwa collaborative recommendation yang dibuat cukup memuaskan."""